1.安装JDK并配置JAVA_HOME 
	JAVA_HOME=/usr/jdk
	HADOOP_HOME=/usr/hadoop
	CLASSPATH=.
	PATH=:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
	export JAVA_HOME
	export HADOOP_HOME
	export CLASSPATH
	export PATH
		
	source .bashrc
2.ssh免密码登陆
	yum -y install openssh-clients
	ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
	cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	ssh 192.168.204.130
	exit
3.配置
	vi /etc/hosts
		192.168.12..128 hive
	vi /usr/hadoop/etc/hadoop/core-site.xml
		<property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://CentOS:9000</value>
		</property>
		<property>
	        <name>hadoop.tmp.dir</name>
	        <value>/usr/hadoop/hadoop-${user.name}</value>
		</property>
	vi /usr/hadoop/etc/hadoop/hdfs-site.xml
		<property>
	        <name>dfs.replication</name>
	        <value>1</value>
		</property>
	vi /usr/hadoop/etc/hadoop/slaves
		hive
4.启动HDFS
	第一次必须格式化namenode（生成fsimage）
	hdfs namenode -format
	start-dfs.sh
	hdfs dfs -chmod -R 777 /
	
Exception:
	ssh: Could not resolve hostname hadoop: Name or service not known
	
		