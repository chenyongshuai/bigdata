0.安装JDK
1.安装Hadoop的HDFS文件系统
2.安装Scala
3.安装Spark
4.配置环境变量
	JAVA_HOME=/usr/jdk
	HADOOP_HOME=/usr/hadoop
	SCALA_HOME=/usr/scala
	SPARK_HOME=/usr/spark
	CLASSPATH=.
	PATH=:$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin
	export JAVA_HOME
	export HADOOP_HOME
	export CLASSPATH
	export PATH
5.vi /usr/spark/conf/spark-env.sh
	export JAVA_HOME=/usr/jdk
	export SCALA_HOME=/usr/scala
	export HADOOP_HOME=/usr/hadoop
	export HADOOP_CONF_DIR=/usr/hadoop/etc/hadoop
	export SPARK_MASTER_IP=192.168.126.141
	export SPARK_WORKER_MEMORY=4g
	export SPARK_WORKER_CORES=2
	export SPARK_WORKER_INSTANCES=1
	export SPARK_DIST_CLASSPATH=$(/usr/hadoop/bin/hadoop classpath)
6.测试
	cd /usr/spark/bin
	./run example sparkPi 10
	
textFile = sc.textFile("file:///usr/spark/README.md")
textFile.filter(lambda line: "Spark" in line).count()
